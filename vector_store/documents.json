[{"content":"# mcp-rag-server\n\nTo install dependencies:\n\n```bash\nbun install\n```\n\nTo run:\n\n```bash\nbun run index.ts\n```\n\nThis project was created using `bun init` in bun v1.2.8. [Bun](https://bun.sh) is a fast all-in-one JavaScript runtime.\n\n## Technical\n\nusing @modelcontextprotocol/sdk\n\n## How RAG Works\n\nThis MCP server implements Retrieval Augmented Generation (RAG) to answer questions based on your documents. Here's the process:\n\n1. **User Query:** You ask a question related to the content of your documents.\n2. **Document Reading:** The server reads `json` `jsonl` `csv` `.txt ` and `.md` files from a path you provide.\n3. **Embedding & Indexing:** It uses an embedding model to tokenize the text and stores these embeddings in a local vector database for efficient searching.\n4. **Search:** When you ask a question, the server searches the vector database for document chunks relevant to your query using embeddings.\n5. **Chunk Selection:** It selects the most relevant chunks (default is 15) based on s","metadata":{"source":"README.md (chunk 1/3)"}},{"content":" a question, the server searches the vector database for document chunks relevant to your query using embeddings.\n5. **Chunk Selection:** It selects the most relevant chunks (default is 15) based on similarity scores.\n6. **Response Generation:** The retrieved chunks are combined with your original question and sent to a Large Language Model (LLM) via the configured API to generate a comprehensive answer.\n\n```mermaid\nflowchart LR\n    A[User asks a question] --> B[RAG mcp server receives query]\n    B --> C{Read/Index Documents (.txt, .md)}\n    C --> D[Search vector database for relevant chunks]\n    D -- Embeddings --> E[Select top 15 chunks]\n    E --> F[Combine query + chunks for LLM]\n    F --> G[Generate Answer via LLM API]\n    G --> H[Return answer to user]\n```\n\n## Environment Variables\n\nTo configure the server, you need to set the following environment variables:\n\n- `BASE_LLM_API`: The base URL for the Large Language Model API endpoint. accept LM studio API, ollama api, openrouter api","metadata":{"source":"README.md (chunk 2/3)"}},{"content":"o configure the server, you need to set the following environment variables:\n\n- `BASE_LLM_API`: The base URL for the Large Language Model API endpoint. accept LM studio API, ollama api, openrouter api, gemini openai compatibility api\n- `LLM_API_KEY`: Your API key for authenticating with the LLM service.\n- `EMBEDDING_MODEL`: Specifies the embedding model used for tokenizing and searching documents (e.g., 'text-embedding-ada-002', 'nomic-embed-text','granite-embedding').\n\nThese variables are crucial for connecting to the necessary AI services.\n\n## Usage Examples\n\n(Provide specific command examples here on how to point the server to document paths and ask questions)\n","metadata":{"source":"README.md (chunk 3/3)"}}]